{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, max_vocab_size=10000, max_seq_length=100):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.word_counts = Counter()\n",
    "        self.vocab_size = 2  # Starting with PAD and UNK tokens\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        # Clean and tokenize all texts\n",
    "        for text in texts:\n",
    "            clean_text = self.clean_text(text)\n",
    "            tokens = clean_text.split()\n",
    "            self.word_counts.update(tokens)\n",
    "        \n",
    "        # Keep only the most common words (minus PAD and UNK which we already have)\n",
    "        vocab_words = [word for word, count in self.word_counts.most_common(self.max_vocab_size - 2)]\n",
    "        \n",
    "        # Create word to index mapping\n",
    "        for word in vocab_words:\n",
    "            self.word2idx[word] = self.vocab_size\n",
    "            self.idx2word[self.vocab_size] = word\n",
    "            self.vocab_size += 1\n",
    "            \n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        \"\"\"Convert texts to sequences of indices\"\"\"\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            clean_text = self.clean_text(text)\n",
    "            tokens = clean_text.split()\n",
    "            # Truncate if longer than max_seq_length\n",
    "            if len(tokens) > self.max_seq_length:\n",
    "                tokens = tokens[:self.max_seq_length]\n",
    "            \n",
    "            # Convert tokens to indices\n",
    "            seq = [self.word2idx.get(word, self.word2idx[\"<UNK>\"]) for word in tokens]\n",
    "            sequences.append(seq)\n",
    "        \n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create PyTorch Dataset\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create collate function for batching\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function to pad sequences in a batch\"\"\"\n",
    "    texts, labels = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    padded_texts = pad_sequence([text for text in texts], batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_texts, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM\n",
    "# Step 4: Build LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=1, \n",
    "                 bidirectional=False, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           )\n",
    "        \n",
    "        # Calculate the size of the output from LSTM\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(lstm_output_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text = [batch size, sentence length]\n",
    "        \n",
    "        # Embed the words\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = [batch size, sentence length, embedding dim]\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # If bidirectional, concatenate the final forward and backward hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        \n",
    "        # Apply dropout\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Training Function\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    # Initialize best validation loss\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_acc = 0\n",
    "        train_samples = 0\n",
    "        \n",
    "        for batch_idx, (texts, labels) in enumerate(train_loader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(texts)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions_class = torch.argmax(predictions, dim=1)\n",
    "            correct = (predictions_class == labels).float().sum()\n",
    "            \n",
    "            # Update metrics\n",
    "            epoch_train_loss += loss.item() * len(labels)\n",
    "            epoch_train_acc += correct.item()\n",
    "            train_samples += len(labels)\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, ' \n",
    "                      f'Loss: {loss.item():.4f}, Acc: {correct.item()/len(labels):.4f}')\n",
    "        \n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        epoch_train_loss /= train_samples\n",
    "        epoch_train_acc /= train_samples\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_valid_loss = 0\n",
    "        epoch_valid_acc = 0\n",
    "        valid_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, labels in valid_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = model(texts)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(predictions, labels)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions_class = torch.argmax(predictions, dim=1)\n",
    "                correct = (predictions_class == labels).float().sum()\n",
    "                \n",
    "                # Update metrics\n",
    "                epoch_valid_loss += loss.item() * len(labels)\n",
    "                epoch_valid_acc += correct.item()\n",
    "                valid_samples += len(labels)\n",
    "        \n",
    "        # Calculate average validation loss and accuracy\n",
    "        epoch_valid_loss /= valid_samples\n",
    "        epoch_valid_acc /= valid_samples\n",
    "        \n",
    "        # Save the best model\n",
    "        if epoch_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = epoch_valid_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'Model saved with validation loss: {best_valid_loss:.4f}')\n",
    "        \n",
    "        # Update history\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        valid_accs.append(epoch_valid_acc)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}')\n",
    "        print(f'Valid Loss: {epoch_valid_loss:.4f}, Valid Acc: {epoch_valid_acc:.4f}')\n",
    "        print('-' * 60)\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'valid_losses': valid_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'valid_accs': valid_accs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluation Function\n",
    "def evaluate_model(model, test_loader, criterion, device, label_names=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(texts)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions_class = torch.argmax(predictions, dim=1)\n",
    "            correct = (predictions_class == labels).float().sum()\n",
    "            \n",
    "            # Collect predictions and labels for classification report\n",
    "            all_predictions.extend(predictions_class.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update metrics\n",
    "            test_loss += loss.item() * len(labels)\n",
    "            test_acc += correct.item()\n",
    "            test_samples += len(labels)\n",
    "    \n",
    "    # Calculate average test loss and accuracy\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "    # Print classification report\n",
    "    if label_names is not None:\n",
    "        print('\\nClassification Report:')\n",
    "        print(classification_report(all_labels, all_predictions, target_names=label_names))\n",
    "    else:\n",
    "        print('\\nClassification Report:')\n",
    "        print(classification_report(all_labels, all_predictions))\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/rotten_tomatoes_movies.csv')\n",
    "df = df.dropna()\n",
    "texts = df['critics_consensus'].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder().fit(df['tomatometer_status'].unique())\n",
    "label_names = le.classes_\n",
    "labels = le.transform(df['tomatometer_status'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Main function to run everything\n",
    "def main():\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Further split training data into train and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # Preprocess text data\n",
    "    preprocessor = TextPreprocessor(max_vocab_size=5000, max_seq_length=50)\n",
    "    preprocessor.fit(X_train)\n",
    "    \n",
    "    # Transform texts to sequences\n",
    "    X_train_seq = preprocessor.transform(X_train)\n",
    "    X_val_seq = preprocessor.transform(X_val)\n",
    "    X_test_seq = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextClassificationDataset(X_train_seq, y_train)\n",
    "    val_dataset = TextClassificationDataset(X_val_seq, y_val)\n",
    "    test_dataset = TextClassificationDataset(X_test_seq, y_test)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    VOCAB_SIZE = preprocessor.vocab_size\n",
    "    EMBEDDING_DIM = 100\n",
    "    HIDDEN_DIM = 128\n",
    "    OUTPUT_DIM = len(set(labels))  # Number of unique classes\n",
    "    N_LAYERS = 2\n",
    "    BIDIRECTIONAL = True\n",
    "    DROPOUT = 0.5\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LSTMClassifier(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        n_layers=N_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(model)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the model\n",
    "    history = train_model(\n",
    "        model=model, \n",
    "        train_loader=train_loader, \n",
    "        valid_loader=val_loader, \n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer, \n",
    "        device=device, \n",
    "        num_epochs=10\n",
    "    )\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc = evaluate_model(\n",
    "        model=model, \n",
    "        test_loader=test_loader, \n",
    "        criterion=criterion, \n",
    "        device=device,\n",
    "        label_names=label_names\n",
    "    )\n",
    "    \n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5000\n",
      "Using device: cpu\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(5000, 100, padding_idx=0)\n",
      "  (lstm): LSTM(\n",
      "    (layers_forward): ModuleList(\n",
      "      (0): LSTMCell(\n",
      "        (W_x): Linear(in_features=100, out_features=512, bias=True)\n",
      "        (W_h): Linear(in_features=128, out_features=512, bias=True)\n",
      "      )\n",
      "      (1): LSTMCell(\n",
      "        (W_x): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (W_h): Linear(in_features=128, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (layers_backward): ModuleList(\n",
      "      (0): LSTMCell(\n",
      "        (W_x): Linear(in_features=100, out_features=512, bias=True)\n",
      "        (W_h): Linear(in_features=128, out_features=512, bias=True)\n",
      "      )\n",
      "      (1): LSTMCell(\n",
      "        (W_x): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (W_h): Linear(in_features=128, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n",
      "Epoch 1/10, Batch 10/182, Loss: 1.0593, Acc: 0.5000\n",
      "Epoch 1/10, Batch 20/182, Loss: 1.0861, Acc: 0.3750\n",
      "Epoch 1/10, Batch 30/182, Loss: 1.0662, Acc: 0.4375\n",
      "Epoch 1/10, Batch 40/182, Loss: 1.2189, Acc: 0.3438\n",
      "Epoch 1/10, Batch 50/182, Loss: 1.0486, Acc: 0.4688\n",
      "Epoch 1/10, Batch 60/182, Loss: 1.0486, Acc: 0.4375\n",
      "Epoch 1/10, Batch 70/182, Loss: 1.0253, Acc: 0.5000\n",
      "Epoch 1/10, Batch 80/182, Loss: 0.9730, Acc: 0.5000\n",
      "Epoch 1/10, Batch 90/182, Loss: 1.0526, Acc: 0.4375\n",
      "Epoch 1/10, Batch 100/182, Loss: 0.9759, Acc: 0.5625\n",
      "Epoch 1/10, Batch 110/182, Loss: 0.9235, Acc: 0.5625\n",
      "Epoch 1/10, Batch 120/182, Loss: 1.0172, Acc: 0.5312\n",
      "Epoch 1/10, Batch 130/182, Loss: 0.9800, Acc: 0.5312\n",
      "Epoch 1/10, Batch 140/182, Loss: 1.0493, Acc: 0.4062\n",
      "Epoch 1/10, Batch 150/182, Loss: 1.0281, Acc: 0.5312\n",
      "Epoch 1/10, Batch 160/182, Loss: 1.0678, Acc: 0.4688\n",
      "Epoch 1/10, Batch 170/182, Loss: 0.9117, Acc: 0.5938\n",
      "Epoch 1/10, Batch 180/182, Loss: 0.9493, Acc: 0.6562\n",
      "Model saved with validation loss: 0.8828\n",
      "Epoch 1/10:\n",
      "Train Loss: 1.0181, Train Acc: 0.4988\n",
      "Valid Loss: 0.8828, Valid Acc: 0.6213\n",
      "------------------------------------------------------------\n",
      "Epoch 2/10, Batch 10/182, Loss: 0.6514, Acc: 0.7812\n",
      "Epoch 2/10, Batch 20/182, Loss: 0.8510, Acc: 0.5000\n",
      "Epoch 2/10, Batch 30/182, Loss: 0.7351, Acc: 0.7188\n",
      "Epoch 2/10, Batch 40/182, Loss: 0.7740, Acc: 0.6562\n",
      "Epoch 2/10, Batch 50/182, Loss: 0.8294, Acc: 0.6250\n",
      "Epoch 2/10, Batch 60/182, Loss: 0.7735, Acc: 0.6875\n",
      "Epoch 2/10, Batch 70/182, Loss: 0.8341, Acc: 0.5938\n",
      "Epoch 2/10, Batch 80/182, Loss: 0.6609, Acc: 0.7500\n",
      "Epoch 2/10, Batch 90/182, Loss: 0.6759, Acc: 0.6875\n",
      "Epoch 2/10, Batch 100/182, Loss: 0.7169, Acc: 0.7500\n",
      "Epoch 2/10, Batch 110/182, Loss: 0.8802, Acc: 0.6250\n",
      "Epoch 2/10, Batch 120/182, Loss: 0.6704, Acc: 0.6562\n",
      "Epoch 2/10, Batch 130/182, Loss: 0.8897, Acc: 0.6250\n",
      "Epoch 2/10, Batch 140/182, Loss: 0.6727, Acc: 0.6875\n",
      "Epoch 2/10, Batch 150/182, Loss: 0.8352, Acc: 0.6562\n",
      "Epoch 2/10, Batch 160/182, Loss: 0.8531, Acc: 0.5625\n",
      "Epoch 2/10, Batch 170/182, Loss: 0.7188, Acc: 0.6875\n",
      "Epoch 2/10, Batch 180/182, Loss: 0.7029, Acc: 0.6875\n",
      "Model saved with validation loss: 0.8048\n",
      "Epoch 2/10:\n",
      "Train Loss: 0.8110, Train Acc: 0.6338\n",
      "Valid Loss: 0.8048, Valid Acc: 0.6538\n",
      "------------------------------------------------------------\n",
      "Epoch 3/10, Batch 10/182, Loss: 0.7671, Acc: 0.7188\n",
      "Epoch 3/10, Batch 20/182, Loss: 0.5764, Acc: 0.7812\n",
      "Epoch 3/10, Batch 30/182, Loss: 0.6843, Acc: 0.7500\n",
      "Epoch 3/10, Batch 40/182, Loss: 0.8029, Acc: 0.6562\n",
      "Epoch 3/10, Batch 50/182, Loss: 1.0444, Acc: 0.5625\n",
      "Epoch 3/10, Batch 60/182, Loss: 0.6734, Acc: 0.6875\n",
      "Epoch 3/10, Batch 70/182, Loss: 0.8470, Acc: 0.6562\n",
      "Epoch 3/10, Batch 80/182, Loss: 0.7237, Acc: 0.6250\n",
      "Epoch 3/10, Batch 90/182, Loss: 0.6980, Acc: 0.7812\n",
      "Epoch 3/10, Batch 100/182, Loss: 0.7682, Acc: 0.6250\n",
      "Epoch 3/10, Batch 110/182, Loss: 0.8264, Acc: 0.5938\n",
      "Epoch 3/10, Batch 120/182, Loss: 0.6571, Acc: 0.7188\n",
      "Epoch 3/10, Batch 130/182, Loss: 0.7913, Acc: 0.5625\n",
      "Epoch 3/10, Batch 140/182, Loss: 0.6221, Acc: 0.8438\n",
      "Epoch 3/10, Batch 150/182, Loss: 0.5000, Acc: 0.8438\n",
      "Epoch 3/10, Batch 160/182, Loss: 0.6273, Acc: 0.8438\n",
      "Epoch 3/10, Batch 170/182, Loss: 0.7522, Acc: 0.7500\n",
      "Epoch 3/10, Batch 180/182, Loss: 0.5556, Acc: 0.7812\n",
      "Epoch 3/10:\n",
      "Train Loss: 0.6567, Train Acc: 0.7193\n",
      "Valid Loss: 0.8182, Valid Acc: 0.6584\n",
      "------------------------------------------------------------\n",
      "Epoch 4/10, Batch 10/182, Loss: 0.5759, Acc: 0.7500\n",
      "Epoch 4/10, Batch 20/182, Loss: 0.3592, Acc: 0.8438\n",
      "Epoch 4/10, Batch 30/182, Loss: 0.5960, Acc: 0.7500\n",
      "Epoch 4/10, Batch 40/182, Loss: 0.6154, Acc: 0.6875\n",
      "Epoch 4/10, Batch 50/182, Loss: 0.5580, Acc: 0.7500\n",
      "Epoch 4/10, Batch 60/182, Loss: 0.3818, Acc: 0.8750\n",
      "Epoch 4/10, Batch 70/182, Loss: 0.5792, Acc: 0.8125\n",
      "Epoch 4/10, Batch 80/182, Loss: 0.4725, Acc: 0.8125\n",
      "Epoch 4/10, Batch 90/182, Loss: 0.5046, Acc: 0.7188\n",
      "Epoch 4/10, Batch 100/182, Loss: 0.5072, Acc: 0.8125\n",
      "Epoch 4/10, Batch 110/182, Loss: 0.6999, Acc: 0.6875\n",
      "Epoch 4/10, Batch 120/182, Loss: 0.5095, Acc: 0.7188\n",
      "Epoch 4/10, Batch 130/182, Loss: 0.4013, Acc: 0.8750\n",
      "Epoch 4/10, Batch 140/182, Loss: 0.4294, Acc: 0.8438\n",
      "Epoch 4/10, Batch 150/182, Loss: 0.6916, Acc: 0.6875\n",
      "Epoch 4/10, Batch 160/182, Loss: 0.4841, Acc: 0.7188\n",
      "Epoch 4/10, Batch 170/182, Loss: 0.6823, Acc: 0.6562\n",
      "Epoch 4/10, Batch 180/182, Loss: 0.4326, Acc: 0.8438\n",
      "Model saved with validation loss: 0.7781\n",
      "Epoch 4/10:\n",
      "Train Loss: 0.5336, Train Acc: 0.7680\n",
      "Valid Loss: 0.7781, Valid Acc: 0.6692\n",
      "------------------------------------------------------------\n",
      "Epoch 5/10, Batch 10/182, Loss: 0.4363, Acc: 0.8750\n",
      "Epoch 5/10, Batch 20/182, Loss: 0.4085, Acc: 0.8750\n",
      "Epoch 5/10, Batch 30/182, Loss: 0.5938, Acc: 0.8125\n",
      "Epoch 5/10, Batch 40/182, Loss: 0.4463, Acc: 0.8438\n",
      "Epoch 5/10, Batch 50/182, Loss: 0.2424, Acc: 0.9375\n",
      "Epoch 5/10, Batch 60/182, Loss: 0.5592, Acc: 0.7188\n",
      "Epoch 5/10, Batch 70/182, Loss: 0.3129, Acc: 0.9062\n",
      "Epoch 5/10, Batch 80/182, Loss: 0.3015, Acc: 0.8438\n",
      "Epoch 5/10, Batch 90/182, Loss: 0.4183, Acc: 0.8125\n",
      "Epoch 5/10, Batch 100/182, Loss: 0.5469, Acc: 0.7812\n",
      "Epoch 5/10, Batch 110/182, Loss: 0.3851, Acc: 0.8125\n",
      "Epoch 5/10, Batch 120/182, Loss: 0.5983, Acc: 0.7812\n",
      "Epoch 5/10, Batch 130/182, Loss: 0.6034, Acc: 0.7500\n",
      "Epoch 5/10, Batch 140/182, Loss: 0.7739, Acc: 0.5938\n",
      "Epoch 5/10, Batch 150/182, Loss: 0.5305, Acc: 0.8125\n",
      "Epoch 5/10, Batch 160/182, Loss: 0.3824, Acc: 0.7812\n",
      "Epoch 5/10, Batch 170/182, Loss: 0.3280, Acc: 0.8438\n",
      "Epoch 5/10, Batch 180/182, Loss: 0.4755, Acc: 0.7500\n",
      "Epoch 5/10:\n",
      "Train Loss: 0.4219, Train Acc: 0.8201\n",
      "Valid Loss: 0.9205, Valid Acc: 0.6136\n",
      "------------------------------------------------------------\n",
      "Epoch 6/10, Batch 10/182, Loss: 0.3462, Acc: 0.9062\n",
      "Epoch 6/10, Batch 20/182, Loss: 0.1250, Acc: 0.9688\n",
      "Epoch 6/10, Batch 30/182, Loss: 0.3049, Acc: 0.8438\n",
      "Epoch 6/10, Batch 40/182, Loss: 0.1978, Acc: 0.9062\n",
      "Epoch 6/10, Batch 50/182, Loss: 0.3370, Acc: 0.8750\n",
      "Epoch 6/10, Batch 60/182, Loss: 0.2729, Acc: 0.8438\n",
      "Epoch 6/10, Batch 70/182, Loss: 0.5654, Acc: 0.7500\n",
      "Epoch 6/10, Batch 80/182, Loss: 0.2263, Acc: 0.9375\n",
      "Epoch 6/10, Batch 90/182, Loss: 0.1892, Acc: 0.9375\n",
      "Epoch 6/10, Batch 100/182, Loss: 0.3395, Acc: 0.7812\n",
      "Epoch 6/10, Batch 110/182, Loss: 0.3330, Acc: 0.8438\n",
      "Epoch 6/10, Batch 120/182, Loss: 0.1437, Acc: 1.0000\n",
      "Epoch 6/10, Batch 130/182, Loss: 0.3457, Acc: 0.8125\n",
      "Epoch 6/10, Batch 140/182, Loss: 0.2535, Acc: 0.9375\n",
      "Epoch 6/10, Batch 150/182, Loss: 0.2032, Acc: 0.9688\n",
      "Epoch 6/10, Batch 160/182, Loss: 0.2688, Acc: 0.9062\n",
      "Epoch 6/10, Batch 170/182, Loss: 0.2626, Acc: 0.9375\n",
      "Epoch 6/10, Batch 180/182, Loss: 0.2531, Acc: 0.8438\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.3332, Train Acc: 0.8633\n",
      "Valid Loss: 1.0376, Valid Acc: 0.6445\n",
      "------------------------------------------------------------\n",
      "Epoch 7/10, Batch 10/182, Loss: 0.2585, Acc: 0.9375\n",
      "Epoch 7/10, Batch 20/182, Loss: 0.3068, Acc: 0.9062\n",
      "Epoch 7/10, Batch 30/182, Loss: 0.1471, Acc: 0.9688\n",
      "Epoch 7/10, Batch 40/182, Loss: 0.1491, Acc: 0.9375\n",
      "Epoch 7/10, Batch 50/182, Loss: 0.3325, Acc: 0.8750\n",
      "Epoch 7/10, Batch 60/182, Loss: 0.1432, Acc: 0.9375\n",
      "Epoch 7/10, Batch 70/182, Loss: 0.2990, Acc: 0.8438\n",
      "Epoch 7/10, Batch 80/182, Loss: 0.0819, Acc: 0.9688\n",
      "Epoch 7/10, Batch 90/182, Loss: 0.3183, Acc: 0.9062\n",
      "Epoch 7/10, Batch 100/182, Loss: 0.0783, Acc: 1.0000\n",
      "Epoch 7/10, Batch 110/182, Loss: 0.1474, Acc: 0.9375\n",
      "Epoch 7/10, Batch 120/182, Loss: 0.0722, Acc: 0.9688\n",
      "Epoch 7/10, Batch 130/182, Loss: 0.3364, Acc: 0.8125\n",
      "Epoch 7/10, Batch 140/182, Loss: 0.1177, Acc: 0.9688\n",
      "Epoch 7/10, Batch 150/182, Loss: 0.2229, Acc: 0.8438\n",
      "Epoch 7/10, Batch 160/182, Loss: 0.2030, Acc: 0.9062\n",
      "Epoch 7/10, Batch 170/182, Loss: 0.1867, Acc: 0.8750\n",
      "Epoch 7/10, Batch 180/182, Loss: 0.2080, Acc: 0.8750\n",
      "Epoch 7/10:\n",
      "Train Loss: 0.2084, Train Acc: 0.9198\n",
      "Valid Loss: 1.2944, Valid Acc: 0.6244\n",
      "------------------------------------------------------------\n",
      "Epoch 8/10, Batch 10/182, Loss: 0.2167, Acc: 0.9688\n",
      "Epoch 8/10, Batch 20/182, Loss: 0.0716, Acc: 1.0000\n",
      "Epoch 8/10, Batch 30/182, Loss: 0.1023, Acc: 0.9688\n",
      "Epoch 8/10, Batch 40/182, Loss: 0.1514, Acc: 0.9688\n",
      "Epoch 8/10, Batch 50/182, Loss: 0.0532, Acc: 1.0000\n",
      "Epoch 8/10, Batch 60/182, Loss: 0.1111, Acc: 0.9375\n",
      "Epoch 8/10, Batch 70/182, Loss: 0.3223, Acc: 0.9062\n",
      "Epoch 8/10, Batch 80/182, Loss: 0.0554, Acc: 0.9688\n",
      "Epoch 8/10, Batch 90/182, Loss: 0.0772, Acc: 0.9688\n",
      "Epoch 8/10, Batch 100/182, Loss: 0.1254, Acc: 0.9375\n",
      "Epoch 8/10, Batch 110/182, Loss: 0.1289, Acc: 0.9375\n",
      "Epoch 8/10, Batch 120/182, Loss: 0.1018, Acc: 1.0000\n",
      "Epoch 8/10, Batch 130/182, Loss: 0.0750, Acc: 0.9688\n",
      "Epoch 8/10, Batch 140/182, Loss: 0.0702, Acc: 0.9375\n",
      "Epoch 8/10, Batch 150/182, Loss: 0.2431, Acc: 0.8750\n",
      "Epoch 8/10, Batch 160/182, Loss: 0.1445, Acc: 0.9062\n",
      "Epoch 8/10, Batch 170/182, Loss: 0.1240, Acc: 0.9375\n",
      "Epoch 8/10, Batch 180/182, Loss: 0.1441, Acc: 0.9375\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.1346, Train Acc: 0.9520\n",
      "Valid Loss: 1.5092, Valid Acc: 0.6631\n",
      "------------------------------------------------------------\n",
      "Epoch 9/10, Batch 10/182, Loss: 0.0416, Acc: 1.0000\n",
      "Epoch 9/10, Batch 20/182, Loss: 0.0693, Acc: 0.9688\n",
      "Epoch 9/10, Batch 30/182, Loss: 0.0624, Acc: 0.9688\n",
      "Epoch 9/10, Batch 40/182, Loss: 0.0218, Acc: 1.0000\n",
      "Epoch 9/10, Batch 50/182, Loss: 0.1945, Acc: 0.9375\n",
      "Epoch 9/10, Batch 60/182, Loss: 0.0397, Acc: 0.9688\n",
      "Epoch 9/10, Batch 70/182, Loss: 0.0278, Acc: 1.0000\n",
      "Epoch 9/10, Batch 80/182, Loss: 0.1252, Acc: 0.9375\n",
      "Epoch 9/10, Batch 90/182, Loss: 0.0238, Acc: 1.0000\n",
      "Epoch 9/10, Batch 100/182, Loss: 0.1312, Acc: 0.9688\n",
      "Epoch 9/10, Batch 110/182, Loss: 0.0143, Acc: 1.0000\n",
      "Epoch 9/10, Batch 120/182, Loss: 0.2381, Acc: 0.9375\n",
      "Epoch 9/10, Batch 130/182, Loss: 0.1530, Acc: 0.9375\n",
      "Epoch 9/10, Batch 140/182, Loss: 0.0759, Acc: 0.9688\n",
      "Epoch 9/10, Batch 150/182, Loss: 0.0805, Acc: 0.9375\n",
      "Epoch 9/10, Batch 160/182, Loss: 0.0359, Acc: 1.0000\n",
      "Epoch 9/10, Batch 170/182, Loss: 0.2270, Acc: 0.9062\n",
      "Epoch 9/10, Batch 180/182, Loss: 0.0923, Acc: 0.9688\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.0848, Train Acc: 0.9718\n",
      "Valid Loss: 1.5091, Valid Acc: 0.6368\n",
      "------------------------------------------------------------\n",
      "Epoch 10/10, Batch 10/182, Loss: 0.1513, Acc: 0.9062\n",
      "Epoch 10/10, Batch 20/182, Loss: 0.0375, Acc: 1.0000\n",
      "Epoch 10/10, Batch 30/182, Loss: 0.0605, Acc: 0.9688\n",
      "Epoch 10/10, Batch 40/182, Loss: 0.0048, Acc: 1.0000\n",
      "Epoch 10/10, Batch 50/182, Loss: 0.0627, Acc: 1.0000\n",
      "Epoch 10/10, Batch 60/182, Loss: 0.0536, Acc: 0.9688\n",
      "Epoch 10/10, Batch 70/182, Loss: 0.0350, Acc: 1.0000\n",
      "Epoch 10/10, Batch 80/182, Loss: 0.0323, Acc: 0.9688\n",
      "Epoch 10/10, Batch 90/182, Loss: 0.0091, Acc: 1.0000\n",
      "Epoch 10/10, Batch 100/182, Loss: 0.0868, Acc: 0.9688\n",
      "Epoch 10/10, Batch 110/182, Loss: 0.0402, Acc: 1.0000\n",
      "Epoch 10/10, Batch 120/182, Loss: 0.0332, Acc: 1.0000\n",
      "Epoch 10/10, Batch 130/182, Loss: 0.0130, Acc: 1.0000\n",
      "Epoch 10/10, Batch 140/182, Loss: 0.0421, Acc: 1.0000\n",
      "Epoch 10/10, Batch 150/182, Loss: 0.0428, Acc: 1.0000\n",
      "Epoch 10/10, Batch 160/182, Loss: 0.1030, Acc: 0.9375\n",
      "Epoch 10/10, Batch 170/182, Loss: 0.1052, Acc: 0.9062\n",
      "Epoch 10/10, Batch 180/182, Loss: 0.1639, Acc: 0.9375\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.0586, Train Acc: 0.9795\n",
      "Valid Loss: 1.7154, Valid Acc: 0.6399\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/tl2fk21s6c7_qd6mtrdkb8480000gn/T/ipykernel_3238/1080640521.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8271, Test Acc: 0.6448\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Certified-Fresh       0.61      0.71      0.65       550\n",
      "          Fresh       0.34      0.24      0.29       382\n",
      "         Rotten       0.80      0.81      0.81       684\n",
      "\n",
      "       accuracy                           0.64      1616\n",
      "      macro avg       0.58      0.59      0.58      1616\n",
      "   weighted avg       0.62      0.64      0.63      1616\n",
      "\n",
      "Final Test Accuracy: 0.6448\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
