no,train_acc,test_acc,train_loss,test_loss,drop_rate,vocab_size,n_heads,embedding_trainable,optimizer,optimizer_lr,weight_decay,cv_fold,epochs,use_stopwords,drop_abbreviation,notes
1,0,0.8780,0,0.6504,0.4,8966,1,False,AdamW,0.001,0.01,9,25,False,False,
2,0,0.9000,0,0.3201,0.4,8966,1,False,AdamW,0.001,0.01,10,15,False,False,change feed forward linear into times 1
3,0,0.8860,0,0.3925,0.6,8966,1,False,AdamW,0.001,0.01,10,15,False,False,change feed forward linear into times 1
4,0,0.8720,0,0.4517,0.6,8966,1,False,AdamW,0.001,0.01,10,15,False,False,change feed forward linear into times 1
5,0,0.8880,0,0.3742,0.4,8966,1,False,AdamW,0.001,0.01,10,15,False,False,change feed forward linear into times 1
6,0,0.9200,0,0.3019,0.4,8966,1,False,AdamW,0.001,0.01,10,15,False,False,change feed forward linear into times 1
7,0,0.8740,0,0.4306,0.7,8966,1,False,AdamW,0.001,0.01,10,15,False,False,change feed forward linear into times 1
8,0,0.8940,0,0.3493,0.2,8966,1,False,AdamW,0.001,0.01,10,15,False,False,change feed forward linear into times 1
9,0,0.8820,0,0.3850,0.2,8966,1,False,AdamW,0.001,0.01,10,15,False,False,change feed forward linear into times 1
10,0,0.8880,0,0.3402,0.2,8966,1,False,AdamW,0.001,0.01,10,15,False,False,change feed forward linear into times 1
11,0,0.8860,0,0.4265,0.2,8966,1,False,AdamW,0.001,0.01,10,15,False,False,change feed forward linear into times 1
12,0,0.9000,0,0.3087,0.2,8966,1,False,AdamW,0.001,0.1,10,15,False,False,change feed forward linear into times 1
13,0,0.9020,0,0.3444,0.2,8966,1,False,AdamW,0.001,0.1,10,15,False,False,change feed forward linear into times 1
14,0,0.9020,0,0.3881,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,change feed forward linear into times 1
15,0,0.8880,0,0.3220,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,change feed forward linear into times 1
16,0,0.8540,0,0.4259,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,change feed forward linear into times 1
17,0,0.8700,0,0.4700,0.5,8966,1,False,AdamW,0.001,0.01,10,20,False,False,change feed forward linear into times 1
18,0,0.8240,0,0.5015,0.5,8966,1,False,AdamW,0.0001,0.01,10,20,False,False,change feed forward linear into times 1
19,0,0.8520,0,0.4417,0.5,8966,1,False,AdamW,0.001,0.01,10,20,False,False,change feed forward linear into times 1
20,0,0.8720,0,0.3866,0.5,8966,1,False,AdamW,0.001,0.01,10,20,False,False,change feed forward linear into times 1
21,0,0.9080,0,0.3387,0.5,8966,1,False,AdamW,0.001,0.01,10,20,False,False,"change feed forward linear into times 1, disable norm1, dropout, and norm2"
22,0,0.9040,0,0.3362,0.2,8966,1,False,AdamW,0.001,0.01,10,20,False,False,"change feed forward linear into times 4, disable norm1, dropout, and norm2"
23,0,0.9020,0,0.3394,0.2,8966,1,False,AdamW,0.001,0.01,10,20,False,False,"change feed forward linear into times 2, disable norm1, dropout, ff, and norm2"
24,0,0.8760,0,0.3358,0.2,8966,1,False,AdamW,0.001,0.01,10,20,False,False,"change feed forward linear into times 2, disable norm1, dropout, ff, and norm2"
25,0,0.8600,0,0.4187,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"change feed forward linear into times 2, disable norm1, dropout, ff, and norm2"
26,0,0.8920,0,0.3083,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, ff, and norm2"
27,0.9589,0.8920,0.1326,0.3083,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, ff, and norm2"
28,0.9763,0.8940,0.0748,0.3240,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff, and norm2"
29,0.9697,0.8900,0.0867,0.3328,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff, and norm2. use relu after attn"
30,0.9791,0.8700,0.0623,0.3644,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
31,0.9933,0.8780,0.0245,0.3458,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
32,0.9952,0.9200,0.0153,0.2766,0.0,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
33,0.9766,0.9040,0.0680,0.3313,0.0,8966,1,False,AdamW,0.001,0.1,10,20,False,False,use ff*2
34,0.9490,0.9080,0.1534,0.2889,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,use ff*2
35,0.8975,0.9140,0.3020,0.2961,0.4,8966,1,False,AdamW,0.001,0.1,10,20,False,False,use ff*2
36,0.8534,0.8960,0.4155,0.3074,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,use ff*2
37,0.8883,0.8960,0.3405,0.2945,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"use ff*2, disable norm1 & norm2"
38,0.9012,0.9120,0.2800,0.2235,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, ff"
39,0.9035,0.9040,0.2726,0.2550,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, ff. add relu after norm2"
40,0.9069,0.9060,0.2671,0.2469,0.5,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, ff. add relu after norm2"
41,0.9720,0.9080,0.0769,0.2862,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, ff. add relu after norm2"
42,0.9731,0.9120,0.0796,0.2457,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
43,0.9681,0.9140,0.0929,0.2841,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
44,0.9692,0.9140,0.0898,0.2715,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff. add relu after norm2"
45,0.9662,0.9260,0.0868,0.2371,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
46,0.9318,0.9200,0.1840,0.2961,0.4,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
47,0.9297,0.9380,0.1955,0.2099,0.4,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
48,0.9327,0.9100,0.1897,0.2452,0.4,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
49,0.9327,0.9100,0.1908,0.2603,0.4,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
50,0.9258,0.9060,0.2202,0.2515,0.4,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2"
51,0.9239,0.9280,0.2243,0.2580,0.4,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2"
52,0.9660,0.9340,0.1085,0.2375,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2"
53,0.9233,0.9200,0.2259,0.2510,0.4,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2"
54,0.9239,0.9240,0.2233,0.2412,0.4,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2"
55,0.9639,0.9340,0.1031,0.2363,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2"
56,0.9455,0.9340,0.1581,0.2296,0.3,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2"
57,0.9621,0.9240,0.1013,0.2507,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2"
58,0.9733,0.9020,0.0783,0.2948,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout, ff"
59,0.9515,0.9200,0.1380,0.2781,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*1"
60,0.9557,0.9100,0.1264,0.2922,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*1"
61,0.9552,0.9080,0.1311,0.3112,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2 with GELU activation function"
62,0.9630,0.9220,0.1069,0.2807,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2 with GELU activation function"
63,0.9658,0.9280,0.1070,0.2340,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2 with GELU activation function"
64,0.9605,0.8940,0.1152,0.2815,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2 with GELU activation function"
65,0.9586,0.9020,0.1223,0.3082,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2 with GELU activation function"
66,0.9637,0.9240,0.1109,0.2539,0.2,8966,1,False,AdamW,0.001,0.1,10,20,False,False,"disable norm1, dropout. use ff*2"
67,0.9467,0.9360,0.1642,0.2253,0.6,8966,1,False,AdamW,0.001,0.1,10,60,False,False,"disable norm1, dropout. use ff*2"
68,0.9446,0.9400,0.1681,0.1861,0.6,8966,1,False,AdamW,0.001,0.1,10,60,False,False,"disable norm1, dropout. use ff*2"
69,0.9021,0.8540,0.2810,0.4525,0.6,8836,1,False,AdamW,0.001,0.1,10,60,True,False,"disable norm1, dropout. use ff*2"
70,0.9416,0.9440,0.1689,0.1761,0.6,8966,1,False,AdamW,0.001,0.1,10,60,False,False,"disable norm1, dropout. use ff*2"
71,0.9683,0.9400,0.1002,0.2029,0.6,8966,1,False,AdamW,0.001,0.1,10,120,False,False,"disable norm1, dropout. use ff*2"
72,0.9407,0.9320,0.1697,0.2690,0.6,8966,1,False,AdamW,0.001,0.1,10,60,False,False,disable norm1. use ff*2
73,0.9324,0.9360,0.2002,0.2207,0.6,8966,1,False,AdamW,0.001,0.1,10,60,False,False,disable dropout. use ff*2
74,0.9341,0.9300,0.1921,0.2918,0.6,8966,1,False,AdamW,0.001,0.1,10,60,False,False,"disable norm2, dropout. use ff*2"
75,0.9527,0.9389,0.1308,0.1698,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,False,"disable norm1, dropout. use ff*2"
76,0.9422,0.9491,0.1610,0.1717,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*1"
77,0.9461,0.9369,0.1600,0.1862,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*1"
78,0.9494,0.9369,0.1324,0.2067,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*3"
79,0.9548,0.9369,0.1292,0.2045,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*4"
80,0.2332,0.1914,1.5978,1.6160,0.6,8898,1,False,Adam,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2"
81,0.9681,0.9430,0.0910,0.2103,0.6,8898,1,False,Adam,0.001,0.0,10,60,False,True,"disable norm1, dropout. use ff*2"
82,0.9708,0.9348,0.0932,0.2096,0.6,8898,1,False,AdamW,0.001,0.0,10,60,False,True,"disable norm1, dropout. use ff*2"
83,0.9501,0.9430,0.1452,0.1797,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2 (BEST MODEL SO FAR)"
84,0.9506,0.9409,0.1408,0.1725,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2"
85,0.8958,0.8656,0.2817,0.4696,0.6,8768,1,False,AdamW,0.001,0.1,10,60,True,True,"disable norm1, dropout. use ff*2"
86,0.9494,0.9328,0.1485,0.1838,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2"
87,0.9934,0.9328,0.0224,0.2314,0.2,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2"
88,0.9753,0.9328,0.0783,0.2966,0.4,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2"
89,0.9569,0.9348,0.1337,0.1883,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2"
90,0.8332,0.9206,0.4674,0.2330,0.8,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2"
91,1.0000,0.9246,0.0001,0.5199,0.0,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2"
92,1.0000,0.9287,0.0000,0.4993,0.0,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2"
93,0.9480,0.9409,0.1580,0.1783,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output"
94,0.9492,0.9369,0.1472,0.1908,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output"
95,0.9525,0.9328,0.1473,0.2017,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output"
96,0.9515,0.9450,0.1428,0.1846,0.6,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output"
97,1.0000,0.9022,0.0001,0.6768,0.0,8898,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output"
98,0.9555,0.9389,0.1344,0.1695,0.6,8897,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove ! from data"
99,0.9513,0.9450,0.1416,0.1745,0.6,8897,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove ! from data"
100,0.9226,0.9002,0.2163,0.2769,0.6,7312,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove ! from data. use stemming"
101,0.9249,0.9022,0.2137,0.2960,0.6,7312,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove ! from data. use stemming"
102,0.9553,0.9246,0.1394,0.2252,0.6,8113,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove ! from data. use lemmatization"
103,0.9544,0.9308,0.1341,0.1972,0.6,8113,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove ! from data. use lemmatization"
104,0.9365,0.9308,0.1784,0.2415,0.6,8113,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove ! from data. use lemmatization"
105,0.9508,0.9308,0.1474,0.1989,0.6,8113,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove ! from data. use lemmatization"
106,0.9520,0.9389,0.1511,0.1975,0.6,8838,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove all punctuation from data"
107,0.9567,0.9348,0.1378,0.1872,0.6,8838,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove all punctuation from data"
108,0.9431,0.9369,0.1597,0.1738,0.6,8838,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove all punctuation from data"
109,0.9024,0.8574,0.2521,0.4276,0.6,7988,1,False,AdamW,0.001,0.1,10,60,True,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove all punctuation from data. use lemmatization and stopwords"
110,0.9031,0.8513,0.2661,0.4664,0.6,7988,1,False,AdamW,0.001,0.1,10,60,True,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove all punctuation from data. use lemmatization and stopwords"
111,0.9511,0.9389,0.1346,0.1554,0.6,8113,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove all punctuation from data. use lemmatization"
112,0.9546,0.9491,0.1317,0.1705,0.6,8113,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove all punctuation from data. use lemmatization"
113,0.9529,0.9267,0.1426,0.1963,0.6,8113,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove all punctuation from data. use lemmatization"
114,0.9518,0.9267,0.1410,0.2150,0.6,8113,1,False,AdamW,0.001,0.1,10,60,False,True,"disable norm1, dropout. use ff*2. split attention layer for forward and backward LSTM output. remove all punctuation from data. use lemmatization"