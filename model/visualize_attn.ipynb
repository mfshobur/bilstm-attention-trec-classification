{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f75c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "from text_preprocessor import TextPreprocessor\n",
    "# from collections import Counter\n",
    "# import gensim\n",
    "# import gensim.downloader\n",
    "# import re\n",
    "# import numpy as np\n",
    "# # from nltk.stem import PorterStemmer\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import string\n",
    "\n",
    "# class TextPreprocessor:\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             max_vocab_size=10000,\n",
    "#             max_seq_length=100,\n",
    "#             embedding_dim=300,\n",
    "#             stopwords: set = None,\n",
    "#             ):\n",
    "#         self.max_vocab_size = max_vocab_size\n",
    "#         self.max_seq_length = max_seq_length\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "#         self.index_to_word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "#         self.word_counts = Counter()\n",
    "#         self.vocab_size = 2  # Starting with PAD and UNK tokens\n",
    "#         self.stopwords = stopwords\n",
    "#         if self.stopwords:\n",
    "#             self.stopwords_intervention()\n",
    "#         self.embedding_matrix = None\n",
    "    \n",
    "#     def get_vocab_from_texts(self, texts, use_stopwords=True):\n",
    "#         # Clean and tokenize all texts\n",
    "#         counter = Counter()\n",
    "#         for text in texts:\n",
    "#             clean_text = self.clean_text(text, use_stopwords)\n",
    "#             tokens = clean_text.split()\n",
    "            \n",
    "#             counter.update(tokens)\n",
    "\n",
    "#         # Keep only the most common words (minus PAD and UNK which we already have)\n",
    "#         # vocab_words = [word for word, count in self.word_counts.most_common(self.max_vocab_size - 2)]\n",
    "#         vocab_words = [word for word, count in counter.most_common()]\n",
    "\n",
    "#         return vocab_words\n",
    "\n",
    "#     def download_model(\n",
    "#             self,\n",
    "#             model_name='word2vec-google-news-300',\n",
    "#             load_from_local=False,\n",
    "#             local_embedding_path='embedding_matrix.npy',\n",
    "#             local_vocab_path='vocab.pkl',\n",
    "#             save_embedding=False,\n",
    "#             filter_from_vocab=None\n",
    "#         ):\n",
    "#         if load_from_local:\n",
    "#             self.load_embedding_matrix(\n",
    "#                 local_embedding_path=local_embedding_path,\n",
    "#                 local_vocab_path=local_vocab_path\n",
    "#             )\n",
    "#         else:\n",
    "#             vectors = gensim.downloader.load(model_name)\n",
    "            \n",
    "#             if filter_from_vocab:\n",
    "#                 self.vocab_size += len(filter_from_vocab)\n",
    "#             else:\n",
    "#                 self.vocab_size += len(vectors.key_to_index)\n",
    "\n",
    "#             self.embedding_matrix = np.zeros((self.vocab_size, self.embedding_dim))\n",
    "\n",
    "#             # set embedding for <UNK>\n",
    "#             self.embedding_matrix[1] = np.random.normal(scale=0.6, size=(self.embedding_dim,))\n",
    "\n",
    "#             if filter_from_vocab:\n",
    "#                 for i, word in enumerate(filter_from_vocab):\n",
    "#                     if vectors.__contains__(word):\n",
    "#                         index = i+2\n",
    "#                         self.embedding_matrix[index] = vectors[word]\n",
    "#                         self.word_to_index[word] = index\n",
    "#                         self.index_to_word[index] = word\n",
    "#             else:\n",
    "#                 for i, word in enumerate(vectors.key_to_index):\n",
    "#                     index = i+2\n",
    "#                     self.embedding_matrix[index] = vectors[word]\n",
    "#                     self.word_to_index[word] = index\n",
    "#                     self.index_to_word[index] = word\n",
    "\n",
    "#             if save_embedding:\n",
    "#                 self.save_embedding_matrix(\n",
    "#                     local_embedding_path=local_embedding_path,\n",
    "#                     local_vocab_path=local_vocab_path\n",
    "#                 )\n",
    "            \n",
    "#             del vectors\n",
    "#             import gc\n",
    "#             gc.collect()\n",
    "    \n",
    "#     def save_embedding_matrix(self,\n",
    "#         local_embedding_path,\n",
    "#         local_vocab_path\n",
    "#         ):\n",
    "#         import pickle\n",
    "#         np.save(local_embedding_path, self.embedding_matrix)\n",
    "#         with open(local_vocab_path, 'wb') as f:\n",
    "#             pickle.dump((self.word_to_index, self.index_to_word), f)\n",
    "\n",
    "\n",
    "#     def load_embedding_matrix(self,\n",
    "#         local_embedding_path,\n",
    "#         local_vocab_path\n",
    "#         ):\n",
    "#         import pickle\n",
    "#         self.embedding_matrix = np.load(local_embedding_path)\n",
    "#         with open(local_vocab_path, 'rb') as f:\n",
    "#             self.word_to_index, self.index_to_word = pickle.load(f)\n",
    "\n",
    "#     def stopwords_intervention(self):\n",
    "#         self.stopwords.add(\"'s\")\n",
    "#         self.stopwords.add(\"'t\")\n",
    "#         self.stopwords.remove('who')\n",
    "#         self.stopwords.remove('where')\n",
    "\n",
    "#     def clean_text(self, text, use_stopwords):\n",
    "#         \"\"\"Clean text: lowercase, remove punctuation, optional stopword removal, and lemmatization\"\"\"\n",
    "\n",
    "#         # Convert to lowercase\n",
    "#         text = text.lower()\n",
    "\n",
    "#         # Remove all punctuation\n",
    "#         text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "#         # Remove extra spaces\n",
    "#         text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "#         # Tokenize text\n",
    "#         tokens = text.split()\n",
    "\n",
    "#         # Initialize lemmatizer\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#         # Remove stopwords and apply lemmatization\n",
    "#         if use_stopwords and self.stopwords:\n",
    "#             tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in self.stopwords]\n",
    "#         else:\n",
    "#             tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#         return \" \".join(tokens)\n",
    "\n",
    "\n",
    "#     def fit(self, texts):\n",
    "#         \"\"\"Build vocabulary from texts\"\"\"\n",
    "#         # Clean and tokenize all texts\n",
    "#         for text in texts:\n",
    "#             clean_text = self.clean_text(text)\n",
    "#             tokens = clean_text.split()\n",
    "#             self.word_counts.update(tokens)\n",
    "\n",
    "#         # Keep only the most common words (minus PAD and UNK which we already have)\n",
    "#         # vocab_words = [word for word, count in self.word_counts.most_common(self.max_vocab_size - 2)]\n",
    "#         vocab_words = [word for word, count in self.word_counts.most_common()]\n",
    "\n",
    "#         # Create word to index mapping\n",
    "#         for word in vocab_words:\n",
    "#             self.word_to_index[word] = self.vocab_size\n",
    "#             self.index_to_word[self.vocab_size] = word\n",
    "#             self.vocab_size += 1\n",
    "\n",
    "#         print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "\n",
    "#     def transform(self, texts, use_stopwords=True):\n",
    "#         \"\"\"Convert texts to sequences of indices\"\"\"\n",
    "#         sequences = []\n",
    "#         for text in texts:\n",
    "#             clean_text = self.clean_text(text, use_stopwords)\n",
    "#             tokens = clean_text.split()\n",
    "#             # Truncate if longer than max_seq_length\n",
    "#             if len(tokens) > self.max_seq_length:\n",
    "#                 tokens = tokens[:self.max_seq_length]\n",
    "\n",
    "#             # Convert tokens to indices\n",
    "#             seq = [self.word_to_index.get(word, self.word_to_index[\"<UNK>\"]) for word in tokens]\n",
    "#             sequences.append(seq)\n",
    "\n",
    "#         return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b05c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "preprocessor.load_embedding_matrix(local_embedding_path='embedding_matrix.npy', local_vocab_path='vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31420ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7353"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "358c41ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7353"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessor.word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be9a45c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from alstm import ALSTMModel\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "cfg = cfg = {\n",
    "    'drop_rate': 0.6,\n",
    "    'output': 5,\n",
    "    'bidirectional': True,\n",
    "    # 'vocab_size': preprocessor.vocab_size,\n",
    "    'vocab_size': 8898,\n",
    "    'context_length': 100,\n",
    "    'emb_dim': 300,\n",
    "    'hidden_size': 150,\n",
    "    'qkv_bias': False,\n",
    "    'n_heads': 75,\n",
    "    'device': device,\n",
    "    'lstm_layers': 1,\n",
    "}\n",
    "\n",
    "model = ALSTMModel(cfg)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('alstm/alstm_002/model_002_0.6/model_002_0.6.pt', map_location=device, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b20d8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import model_view\n",
    "from transformers import AutoTokenizer, AutoModel, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ada0c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 847, 58]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'who invent war'\n",
    "\n",
    "idx = preprocessor.transform([text])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4d3eab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['who', 'invent', 'war']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.retransform(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4a2da66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0219, -0.2024,  0.1386,  0.3003,  0.4489, -0.0135, -0.2492,\n",
       "           0.1243, -0.0906,  0.4554,  0.1734, -0.3435,  0.8124,  0.3900,\n",
       "          -0.5851, -1.0427, -0.4524, -0.1193, -0.0941,  0.0099, -0.1597,\n",
       "           0.1230,  1.3998,  0.2652,  2.1271,  0.7677,  0.2417, -0.0315,\n",
       "          -0.6147, -0.3794, -1.3430, -0.8605,  0.4050, -0.6411,  0.1951,\n",
       "           0.0514, -0.8814,  0.5694, -0.3655, -1.5260,  0.0728,  0.9261,\n",
       "          -0.4259, -0.8872,  0.7500,  0.0802,  0.4781, -0.3833, -0.2083,\n",
       "           0.7117, -0.2813, -0.1287, -0.4637, -1.3941,  0.1508,  0.5293,\n",
       "          -0.3861,  0.0166,  1.3384, -0.6833, -1.0763, -0.1982, -0.3693,\n",
       "           1.1161, -0.0882, -0.7264, -0.4425, -0.4601,  0.0106,  0.1179,\n",
       "          -1.1874, -1.1754,  0.1764,  0.4424, -0.2770,  0.0576,  0.7497,\n",
       "           0.6138, -0.2391,  0.8687, -0.3052,  0.1988, -1.5313,  0.0595,\n",
       "           0.1230,  0.1123,  1.0281, -0.2567,  0.6372, -0.8656,  0.2747,\n",
       "           0.2824, -0.4641, -0.1316,  1.4507,  0.1538,  0.1064, -0.2024,\n",
       "           0.0564,  0.0690, -0.0900,  0.3218,  0.3357,  0.4078,  0.1447,\n",
       "           0.4616, -0.9328,  0.2881, -0.0272, -0.0481,  0.2728,  0.3300,\n",
       "          -0.2805,  0.1677, -0.0572, -0.6502,  0.3995, -0.6002, -0.7467,\n",
       "           0.7212, -0.7996,  0.1305, -0.4209,  0.6829,  0.0738, -0.4612,\n",
       "          -0.4396,  0.0576, -0.7368,  0.9233,  1.2841, -0.3393,  1.0019,\n",
       "           0.3152, -0.2833,  1.3837, -0.3334,  1.1890,  0.6029,  0.1894,\n",
       "           0.5402, -0.6373,  1.1102,  0.6557, -0.9688, -0.1551, -0.2837,\n",
       "          -0.7045, -1.3328, -0.1791],\n",
       "         [ 0.0208,  0.1396,  0.3407,  0.3564,  0.4305, -0.0124, -0.0185,\n",
       "           0.0891,  0.0321,  0.4151,  0.1551, -0.2506,  1.0358,  0.4345,\n",
       "          -0.8565, -1.9120, -0.0898,  0.0336, -0.3070, -0.1955, -0.0700,\n",
       "           0.0226,  1.5525,  0.2463,  2.8383,  0.7956, -0.0625,  1.0683,\n",
       "          -0.6854, -0.5687, -1.1801, -1.3053, -0.0411, -0.8545,  0.0925,\n",
       "          -0.1792, -1.0281,  0.2370, -0.3681, -1.8539,  0.0169,  1.2127,\n",
       "          -0.3432, -1.2138,  0.7343,  0.1267,  0.7077, -0.5275, -0.4030,\n",
       "           1.0695, -0.1577,  0.1758, -0.1781, -1.7651,  0.5570,  0.4317,\n",
       "          -0.2151,  0.3603,  1.4802, -0.6249, -0.7517, -0.2800, -0.4930,\n",
       "           0.6034, -0.1255, -0.6599, -0.3659, -0.6130, -0.1100, -0.0463,\n",
       "          -1.2044, -1.3153, -0.1112,  0.7474, -0.2889,  0.4300,  0.6808,\n",
       "           0.4873, -0.3819,  1.1863, -0.7200,  0.6880, -2.3803,  0.1531,\n",
       "          -0.0247,  0.1133,  1.5956, -0.4094,  0.5976, -0.6871,  0.0931,\n",
       "           0.2150, -0.6015, -0.2044,  1.3477,  0.1486,  0.0477, -0.5880,\n",
       "           0.4868,  0.6209,  0.5240,  0.6249,  0.1095,  1.1971,  0.0371,\n",
       "           0.2584, -0.8503,  0.3913, -0.4589, -0.2012,  0.5583,  0.3410,\n",
       "          -0.2233,  0.2749, -0.2579, -0.7897, -0.0525, -0.7211, -1.3092,\n",
       "           0.7854, -0.9484,  0.2538, -0.7866,  0.7411,  0.0787, -0.4908,\n",
       "          -0.4599,  0.2060, -1.1726,  0.8375,  1.2623, -0.2854,  1.0947,\n",
       "           0.7880, -0.3069,  2.2114, -0.6196,  2.1087,  0.3940, -0.1741,\n",
       "           0.6837, -0.6504,  0.9187,  0.7439, -1.1918,  0.0887, -0.1117,\n",
       "          -1.8631, -1.6944, -0.1775],\n",
       "         [ 0.1591,  0.3154,  0.3492,  0.0568,  0.3648, -0.0106, -0.1871,\n",
       "           0.0674,  0.4141,  0.5635,  0.0963,  0.1806,  1.2877,  0.2260,\n",
       "          -0.7821, -2.5335,  0.1215,  0.5533, -0.3607, -0.8576, -0.6406,\n",
       "          -0.0711,  1.6405,  0.0814,  3.4473,  1.4259, -0.3135,  1.4651,\n",
       "          -0.8453, -0.3573, -1.2024, -1.8488, -0.3443, -0.7677,  0.1773,\n",
       "          -0.1219, -1.0077, -0.2717, -0.4708, -2.2102, -0.1962,  1.0599,\n",
       "          -0.4682, -1.2842,  0.7902,  0.2951,  0.6520, -0.6079, -0.2107,\n",
       "           1.7759, -0.0503,  0.0650, -0.1375, -1.9766,  0.7557,  0.3533,\n",
       "          -0.3630,  0.1225,  1.5817, -0.7273, -0.8157, -0.1510, -0.2536,\n",
       "           0.6631, -0.4380, -1.1242, -0.8671, -0.4077, -0.1333, -0.2933,\n",
       "          -1.1368, -1.7126,  0.0168,  0.4275, -0.3140,  0.5246,  0.9229,\n",
       "          -0.0832, -0.2839,  1.7347, -1.2260,  1.1818, -2.5576,  0.4132,\n",
       "           0.0236,  0.3579,  1.9514, -0.5786,  0.7176, -0.9286,  0.0342,\n",
       "          -0.0662, -0.8131, -0.2469,  1.8790,  0.0491, -0.0906, -1.4163,\n",
       "           0.6843,  0.8137,  0.7317,  0.6264,  0.5207,  1.5837,  0.1140,\n",
       "           0.5595, -0.7019,  0.2513, -0.4240, -0.3275,  0.3162,  0.3714,\n",
       "          -0.1585,  0.1697, -0.6072, -0.9373,  0.0755, -0.8540, -1.8172,\n",
       "           0.9971, -0.9880,  0.5565, -0.5490,  0.6573,  0.4000, -0.2521,\n",
       "          -0.3773,  0.0808, -1.1718,  0.6971,  1.2340, -0.2398,  0.6615,\n",
       "           0.5425, -0.4486,  3.0509, -0.8196,  2.6531,  0.1851, -0.8397,\n",
       "           0.4727, -0.7163,  1.3258,  1.1986, -1.2964,  0.1351, -0.2968,\n",
       "          -2.6795, -1.6657, -0.3017]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(torch.tensor(idx), output_attention=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02f3a526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 150])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4332e2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 150])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(output, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45b15b39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai-env/lib/python3.11/site-packages/bertviz/model_view.py:67\u001b[39m, in \u001b[36mmodel_view\u001b[39m\u001b[34m(attention, tokens, sentence_b_start, prettify_tokens, display_mode, encoder_attention, decoder_attention, cross_attention, encoder_tokens, decoder_tokens, include_layers, include_heads, html_action)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     66\u001b[39m     include_heads = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_heads))\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m attention = \u001b[43mformat_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sentence_b_start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m     attn_data.append(\n\u001b[32m     70\u001b[39m         {\n\u001b[32m     71\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m         }\n\u001b[32m     76\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai-env/lib/python3.11/site-packages/bertviz/util.py:11\u001b[39m, in \u001b[36mformat_attention\u001b[39m\u001b[34m(attention, layers, heads)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_attention \u001b[38;5;129;01min\u001b[39;00m attention:\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# 1 x num_heads x seq_len x seq_len\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_attention.shape) != \u001b[32m4\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe attention tensor does not have the correct number of dimensions. Make sure you set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m                          \u001b[33m\"\u001b[39m\u001b[33moutput_attentions=True when initializing your model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m     layer_attention = layer_attention.squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m heads:\n",
      "\u001b[31mValueError\u001b[39m: The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model."
     ]
    }
   ],
   "source": [
    "model_view(torch.unsqueeze(output, dim=0), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9ef8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
