{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell_v1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # forget gate\n",
    "        self.W_if = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hf = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # input gate\n",
    "        self.W_ii = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hi = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_ig = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hg = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # output gate\n",
    "        self.W_io = nn.Linear(input_size, hidden_size)\n",
    "        self.W_ho = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    \n",
    "    def forget_gate(self, x, h):\n",
    "        sum = self.W_if(x) + self.W_hf(h)\n",
    "        return torch.sigmoid(sum)\n",
    "    \n",
    "    def input_gate(self, x, h):\n",
    "        sum_i = self.W_ii(x) + self.W_hi(h)\n",
    "        \n",
    "        return torch.sigmoid(sum_i)\n",
    "    \n",
    "    def gate_gate(self, x, h):\n",
    "        sum_g = self.W_ig(x) + self.W_hg(h)\n",
    "        return torch.tanh(sum_g)\n",
    "\n",
    "    def output_gate(self, x, h):\n",
    "        sum = self.W_io(x) + self.W_ho(h)\n",
    "        return torch.sigmoid(sum)\n",
    "    \n",
    "    def forward(self, x, hx):\n",
    "        \"\"\"\n",
    "        hx[0] = h0 (previous hidden state)\n",
    "        hx[1] = c0 (previous cell state)\n",
    "        \"\"\"\n",
    "        f = self.forget_gate(x, hx[0])\n",
    "\n",
    "        i = self.input_gate(x, hx[0])\n",
    "\n",
    "        g = self.gate_gate(x, hx[0])\n",
    "\n",
    "        c = f * hx[1] + i * g\n",
    "\n",
    "        o = self.output_gate(x, hx[0])\n",
    "\n",
    "        h = o * torch.tanh(c)\n",
    "\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0136,  0.0510], grad_fn=<MulBackward0>),\n",
       " tensor([-0.0877,  0.0929], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = LSTMCell_v1(3, 2)\n",
    "\n",
    "input = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "h0 = torch.zeros(2)\n",
    "c0 = torch.zeros(2)\n",
    "\n",
    "output = cell(input, (h0, c0))\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Cell Efficient Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.W_x = nn.Linear(input_size, 4 * hidden_size) # 4 is for forget, input, gate, and output gate\n",
    "        self.W_h = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "    \n",
    "    def forward(self, x, hx):\n",
    "        assert x.shape[-1] == self.W_x.in_features, \"Input size mismatch\"\n",
    "        assert hx[0].shape[-1] == self.W_x.out_features // 4, \"Output size mismatch\"\n",
    "\n",
    "        gates = self.W_x(x) + self.W_h(hx[0])\n",
    "        f, i, g, o = torch.chunk(gates, 4, dim=-1)\n",
    "        f = torch.sigmoid(f)\n",
    "        i = torch.sigmoid(i)\n",
    "        g = torch.tanh(g)\n",
    "        o = torch.sigmoid(o)\n",
    "\n",
    "        c = f * hx[1] + i * g\n",
    "\n",
    "        h = o * torch.tanh(c)\n",
    "\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1761, 0.0148, 0.4325], grad_fn=<MulBackward0>),\n",
       " tensor([0.7827, 0.1725, 0.5500], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = LSTMCell(5, 3)\n",
    "\n",
    "input = torch.tensor([1., 2., 3., 4., 5.,])\n",
    "\n",
    "h0 = torch.zeros(3)\n",
    "c0 = torch.zeros(3)\n",
    "\n",
    "output = cell(input, (h0, c0))\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1195, -0.2427, -0.9611,  0.5320, -2.3102],\n",
       "         [ 0.4877,  1.0431,  0.8586, -0.3078,  2.6141],\n",
       "         [ 0.2589, -0.9733, -1.6025,  1.4277,  0.6385],\n",
       "         [-0.7495,  0.5077, -0.1007, -0.6688,  0.1554]],\n",
       "\n",
       "        [[ 1.0665,  0.2208,  1.8834, -1.1158, -0.2876],\n",
       "         [ 1.5418, -1.6930, -1.5466, -0.6099, -0.0300],\n",
       "         [ 0.0327, -0.7160,  1.0028,  0.9985,  0.7949],\n",
       "         [-2.0103,  0.2070, -0.9969, -1.3104, -0.7697]],\n",
       "\n",
       "        [[-0.2051,  0.0206, -0.3492,  0.2328,  1.5664],\n",
       "         [ 0.5154,  0.4913, -0.6668, -0.8935, -0.5981],\n",
       "         [ 0.7033, -0.9606,  1.4943,  0.4578, -0.7708],\n",
       "         [ 0.6048, -0.2460,  1.2659,  0.9342, -0.8268]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3, 4, 5)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1999, -0.0150, -0.1023],\n",
       "        [-0.1042, -0.0923, -0.0475],\n",
       "        [-0.0583,  0.0196,  0.0155]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = LSTMCell(5, 3)\n",
    "\n",
    "# 3 batch, 4 input_size/sequence_length, 5 emb_size\n",
    "# input = torch.tensor([[1., 2., 3., 4., 5.,], [1., 2., 3., 4., 5.,], [1., 2., 3., 4., 5.,]])\n",
    "input = torch.randn(3, 4, 5)\n",
    "\n",
    "h0 = torch.zeros(3)\n",
    "c0 = torch.zeros(3)\n",
    "\n",
    "ht, ct = cell(input[:,0], (h0, c0))\n",
    "\n",
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1023, -0.0475,  0.0155], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_v1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.0, bidirectional=False, device=None):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "\n",
    "        # create layers\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        if self.bidirectional:\n",
    "            self.backward_layers = torch.nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(LSTMCell(self.input_size, self.hidden_size))\n",
    "                if self.bidirectional:\n",
    "                    self.backward_layers.append(LSTMCell(self.input_size, self.hidden_size))\n",
    "            else:\n",
    "                self.layers.append(LSTMCell(self.hidden_size, self.hidden_size))\n",
    "                if self.bidirectional:\n",
    "                    self.backward_layers.append(LSTMCell(self.hidden_size, self.hidden_size))\n",
    "            \n",
    "        self.num_layers = num_layers * self.num_directions\n",
    "\n",
    "    def forward(self, x: torch.tensor, h0: tuple[torch.tensor, torch.tensor] = None):\n",
    "        assert x.shape[-1] == self.input_size, \"Input size mismatch\"\n",
    "        if h0 is not None:\n",
    "            assert h0[0].shape[-1] == self.hidden_size, \"Output size mismatch\"\n",
    "\n",
    "        seq_length = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        if h0 is None:\n",
    "            hx, cx = torch.chunk(torch.zeros(self.num_layers*2, batch_size, self.hidden_size), 2)\n",
    "        else:\n",
    "            hx, cx = h0\n",
    "\n",
    "        # output = torch.tensor([]) # contains all hidden state for the last layer (batch_size, seq_length, hidden_size * D)\n",
    "        h_n = torch.tensor([]) # contains all last hidden state for every layer (num_layers * D, batch_size, hidden_size)\n",
    "        c_n = torch.tensor([])  # contains all last cell state for every layer (num_layers * D, batch_size, hidden_size)\n",
    "        # output_backward = torch.tensor([])\n",
    "        h_n_backward = torch.tensor([])\n",
    "        c_n_backward = torch.tensor([])\n",
    "\n",
    "        input_backward = None\n",
    "        ht_backward = None\n",
    "\n",
    "        \n",
    "        # loop layers\n",
    "        for i in range(len(self.layers)):\n",
    "            next_layer_input = torch.tensor([])\n",
    "\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                if self.bidirectional:\n",
    "                    input_backward = x.flip(dims=(1,))\n",
    "                    next_layer_input_backward = torch.tensor([])\n",
    "            else:\n",
    "                if self.bidirectional:\n",
    "                    next_layer_input_backward = torch.tensor([])\n",
    "                \n",
    "            ht, ct = hx[i], cx[i]\n",
    "            ht_backward, ct_backward = hx[self.num_layers//2+i], cx[self.num_layers//2+i]\n",
    "            for j in range(seq_length):\n",
    "                ht, ct = self.layers[i](input[:,j], (ht,ct)) # take input on [all batch, current seq length]\n",
    "                next_layer_input = torch.cat([next_layer_input, ht.unsqueeze(1)], dim=1)\n",
    "                # hx = ht.clone().detach()\n",
    "\n",
    "                if self.bidirectional:\n",
    "                    # ht_backward, ct_backward = self.backward_layers[i](input_backward[:,j], (ht[i+1],ct[i+1]))\n",
    "                    ht_backward, ct_backward = self.backward_layers[i](input_backward[:,j], (ht_backward,ct_backward))\n",
    "                    next_layer_input_backward = torch.cat([next_layer_input_backward, ht_backward.unsqueeze(1)], dim=1)\n",
    "                    # input_backward = ht_backward\n",
    "            \n",
    "            # append the h_n output of hidden state where n = seq_length on every layer\n",
    "            h_n = torch.cat([h_n, ht.unsqueeze(0)])\n",
    "            c_n = torch.cat([c_n, ct.unsqueeze(0)])\n",
    "            if self.bidirectional:\n",
    "                h_n_backward = torch.cat([h_n_backward, ht_backward.unsqueeze(0)])\n",
    "                c_n_backward = torch.cat([c_n_backward, ct_backward.unsqueeze(0)])\n",
    "\n",
    "            input = next_layer_input.clone().detach()\n",
    "            if self.bidirectional:\n",
    "                input_backward = next_layer_input_backward.clone().detach()\n",
    "\n",
    "        # output = ht\n",
    "        output = next_layer_input.clone().detach()\n",
    "        if ht_backward is not None:\n",
    "            output_backward = next_layer_input_backward.clone().detach()\n",
    "            output = torch.cat([output, output_backward], dim=2)\n",
    "            h_n = torch.cat([h_n, h_n_backward], dim=0)\n",
    "        \n",
    "        assert output.shape == (batch_size, seq_length, self.hidden_size * self.num_directions), \"Output shape mismatch\"\n",
    "        assert h_n.shape == (len(self.layers) * self.num_directions, batch_size, self.hidden_size), \"Hidden state mismatch\"\n",
    "        return output, h_n, c_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 8]), torch.Size([4, 2, 4]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "input_size = 6\n",
    "hidden_size = 4\n",
    "num_layers = 2\n",
    "seq_length = 3\n",
    "\n",
    "lstm = LSTM_v1(input_size, hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "inputs = torch.randn(batch_size, seq_length, input_size)\n",
    "# hx = torch.zeros(4)\n",
    "# cx = torch.zeros(4)\n",
    "\n",
    "# output, h_n, c_n = lstm(inputs, (hx, cx))\n",
    "output, h_n, c_n = lstm(inputs)\n",
    "output.shape, h_n.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- output shape should be (batch_size, seq_length, hidden_size * D) -> (2, 3, 4*2) which is true\n",
    "- hidden state shape should be (num_layers * D, batch_size, hidden_size) -> (2*2, 2, 4) which is true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
