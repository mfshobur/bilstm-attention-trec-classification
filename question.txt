LAKUKAN UJI COBA MODEL TANPA ATTENTION DENGAN HYPERPARAMETER GRIDSEARCH TERBAIK

Jum'at, 21-03-2025

1. 
bagaimana cara menghandle token 's dan 't pada data? misalnya pada kalimat:
    What was the name of Robert Fulton 's most famous steamboat ?
dan pada kalimat:
    What 's the name of the Wilkes plantation in Gone with the Wind ?
apakah dibiarkan saja atau dihapus
= gunakan stopwords


2. di bagian mana dropout pada LSTM sebaiknya ditambahkan
= setelah LSTM


3. apakah penggunaan stopwords aman dilakukan pada data (terkhusus data klasifikasi pertanyaan) ('t, 's, not)
= bisa

4. gunakan stopword exclude (who, where)

5. cari tahu kata ganti he/she di entity 
= terdapat 8 kata he/she di entity


sudah dikerjakan:
1. explore data (cleaning belum rampung)
2. buat embedding
2. bangun LSTM cell from scratch
3. bangun modul LSTM from scratch


future works:
1. bangun modul attention from scratch (DONE)
2. bangun model LSTM + attention (multihead attention) (DONE)
3. training model (variasi jumlah layer attention) 
4. pada setiap variasi model, perhatikan bagaimana setiap layer attention menghubungkan kata

distribusi data/kata setelah preprocess

note:
optimizer yang digunakan adalah Adam (coba ditelusuri)
loss function yang digunakan adalah Cross Entropy
pada kategori pertanyaan entity, tidak terdapat pertanyaan yang bersinggungan dengan kategori human
embedding yang digunakan adalah word2vec
embedding dimension adalah 300

note:
optimizer yang digunakan adalah Adam (coba ditelusuri)
DONE sub bab stopwords intervention setelah preprocess (tdk smua stopword diterima)
jelaskan deskripsi masing" kategori

COARSE_LABEL = 
[
    0 = "ABBR",
    1 = "ENTY",
    2 = "DESC",
    3 = "HUM",
    4 = "LOC",
    5 = "NUM"
]

COARSE_LABEL = 
[
    0 = "ENTY",
    1 = "DESC",
    2 = "HUM",
    3 = "LOC",
    4 = "NUM"
]





29 APR 2025
todo:
- sertakan sitasi gpt2
- tampilkan tabel confusion matrix dengan data quantity
- lanjut uji coba jumlah attention heads
- mulai menulis mengenai data, data intervention, not using stopwords, model, model inspiration (gpt-2)

revise:
-

30 April 2025
task:
- train current model with variety of dropout value

=================================================
5 May 2025

code:
- ambil model early stopping
- vocabulary size
- tampilkan visualisasi heatmap attention (best & worst model)
- tumpuk multi head attention
- uji coba epoch, dropout, learning rate

penulisan:
- menghindari stopwrod krna kalimat tanya adalah stopword list
- fokus ke dropout 0.6
- jelaskan skenario data preprocessing (tabel) 

question for next meet:
code:
- masked multi head attention, hapus mask-nya?


penulisan:
- apakah backpropagation harus dijelaskan lebih lanjut
- singkatan
- penggunaan kata data training/data latih
- cek semua kata data apakah dicetak miring


what to prepare for scenario training:
- implement early stopping
- implement looping, save the best model




--